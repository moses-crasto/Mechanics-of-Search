This project focuses on implementing and evaluating an Information Retrieval (IR) system using the Cranfield Collection, a dataset comprising 1,400 abstracts and associated queries. The primary objectives include indexing the document collection, implementing three different retrieval models (Vector Space Model, BM25, and Query Likelihood), and evaluating their performance using standard metrics such as Mean Average Precision (MAP), Precision at 5 (P@5), and Normalized Discounted Cumulative Gain (NDCG). 
The indexing phase involves preprocessing the documents, constructing an inverted index, and organizing the data structures for efficient retrieval. The retrieval models are then implemented, each with its unique approach to ranking documents based on their relevance to user queries. 
For evaluation, the TREC evaluation program is utilized, and the results are compared across the different retrieval models. The findings provide insights into the effectiveness of each model in retrieving relevant documents from the Cranfield Collection.
This report presents a comprehensive overview of the project, detailing the methodology, implementation choices, evaluation process, and results. The analysis of the evaluation outcomes sheds light on the strengths and weaknesses of the retrieval models, offering valuable insights for future improvements and extensions to the IR system.
